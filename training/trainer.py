import torch
import os
import torch.nn as nn
import torch.nn.functional as F
import math
import time
import json
import json
from pathlib import Path
from torch.utils.data import DataLoader
from torch.amp import autocast
from tqdm import tqdm
from typing import List, Optional, Callable, Dict, Any
from collections import defaultdict
from configs.llm_config import BlueberryConfig
from models.llm import MinimalLLM
from optimizers.muon import Muon
from training.evaluation import evaluate_model
from utils.helpers import set_seed, format_time
from utils.spectral import (
    compute_spectral_stats, 
    compute_singular_values,
    compute_subspace_alignment,
    compute_spectral_entropy,
    compute_orthogonality_error
)


class EarlyStopping:
    """Early stopping handler"""
    def __init__(self, patience: int = 30, min_delta: float = 0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0
        self.best_step = 0
        
    def __call__(self, val_loss: float, step: int) -> bool:
        """Returns True if training should stop"""
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.best_step = step
            self.counter = 0
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                print(f"\n‚èπÔ∏è  Early stopping triggered at step {step}")
                print(f"   Best loss: {self.best_loss:.4f} at step {self.best_step}")
                return True
            return False



def setup_muon_optimizer(model: nn.Module, config: BlueberryConfig):
    """Setup Muon optimizer with hybrid approach"""
    muon_params = []
    adamw_params = []

    for name, param in model.named_parameters():
        if (param.ndim == 2 and 
            'token_embedding' not in name and 
            'norm' not in name and 
            param.requires_grad):
            muon_params.append(param)
        else:
            adamw_params.append(param)

    print(f"  Muon parameters: {sum(p.numel() for p in muon_params):,}")
    print(f"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}")

    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=config.muon_momentum)
    adamw_optimizer = torch.optim.AdamW(
        adamw_params,
        lr=config.adamw_lr,
        weight_decay=config.weight_decay,
        fused=torch.cuda.is_available()
    )

    return [muon_optimizer, adamw_optimizer]


def train_model(
    model: nn.Module,
    config: BlueberryConfig,
    train_loader: DataLoader,
    val_loader: DataLoader,
    optimizers: List[torch.optim.Optimizer],
    schedulers: Optional[List] = None,
    early_stopper: Optional[EarlyStopping] = None,
    output_dir: Optional[str] = None,
    extra_config: Optional[Dict[str, Any]] = None,
    log_every: int = 100,
    track_manifold: bool = False,
    checkpoint_dir: Optional[str] = "checkpoints",
    checkpoint_every: int = 5000,
    resume_from: Optional[str] = None,
) -> Any:
    """
    Generic training function that can be used by experiments.
    
    Args:
        model: Model to train
        config: Model configuration
        train_loader: Training data loader
        val_loader: Validation data loader
        optimizers: List of optimizers
        schedulers: Optional list of learning rate schedulers
        early_stopper: Optional early stopping handler
        output_dir: Optional directory to save outputs
        extra_config: Optional dict of extra config to save with metrics
    
    Returns:
        model, final_metrics, metrics_history
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device, dtype=torch.bfloat16)
    
    if schedulers is None:
        schedulers = []

    current_loss_val = 0.0

    # Training metrics tracking
    # Synchronize CUDA to ensure accurate timing (no queued operations)
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    train_start_time = time.time()
    metrics_history = {
        'steps': [],
        'val_losses': [],
        'val_accuracies': [],
        'val_perplexities': [],
        'elapsed_times': [],
        'learning_rates': [],
    }
    
    if track_manifold:
        metrics_history['manifold_history'] = defaultdict(list)

    # Resume from checkpoint if specified
    start_step = 0
    start_tokens = 0
    if resume_from and os.path.exists(resume_from):
        print(f"üîÑ Resuming from checkpoint: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        for i, opt in enumerate(optimizers):
            opt.load_state_dict(checkpoint['optimizer_states'][i])
        for i, sch in enumerate(schedulers):
            sch.load_state_dict(checkpoint['scheduler_states'][i])
        metrics_history = checkpoint['metrics_history']
        start_step = checkpoint['step']
        start_tokens = checkpoint['tokens_seen']
        print(f"   Resumed at step {start_step}, tokens {start_tokens:,}")

    # Training loop
    model.train()
    step = start_step
    tokens_seen = start_tokens
    desc = "Training"
    pbar = tqdm(total=config.train_tokens, desc=desc, unit="tokens", initial=tokens_seen)
    
    stopped_early = False

    while tokens_seen < config.train_tokens:
        for batch_idx, batch in enumerate(train_loader):
            if tokens_seen >= config.train_tokens:
                break

            # Handle different batch formats
            if isinstance(batch, dict):
                x = batch["input_ids"]
                y = batch["labels"]
                attention_mask = batch.get("attention_mask")
            elif isinstance(batch, (list, tuple)):
                if len(batch) == 3:
                    x, attention_mask, y = batch
                elif len(batch) == 2:
                    x, y = batch
                    attention_mask = None
                else:
                    raise ValueError(f"Unexpected batch structure with {len(batch)} elements.")
            else:
                raise TypeError(f"Unsupported batch type: {type(batch)}")

            x, y = x.to(device), y.to(device)
            if attention_mask is not None:
                attention_mask = attention_mask.to(device)
            
            # Count tokens in this batch (approx: batch_size * seq_len)
            batch_tokens = x.numel()

            # Forward pass (optimized to avoid large contiguous copies of logits)
            if config.use_amp:
                with autocast('cuda', dtype=torch.bfloat16):
                    logits = model(x)
                    # Shift labels instead of logits to save ~3GB VRAM
                    # We set the last token to -100 so cross_entropy ignores it
                    shift_labels = torch.full_like(y, -100)
                    shift_labels[:, :-1] = y[:, 1:]
                    
                    ce_loss = F.cross_entropy(
                        logits.view(-1, config.vocab_size),
                        shift_labels.view(-1),
                        ignore_index=-100
                    )
                    loss = ce_loss / config.gradient_accumulation_steps
                loss.backward()
            else:
                logits = model(x)
                shift_labels = torch.full_like(y, -100)
                shift_labels[:, :-1] = y[:, 1:]
                
                ce_loss = F.cross_entropy(
                    logits.view(-1, config.vocab_size),
                    shift_labels.view(-1),
                    ignore_index=-100
                )
                loss = ce_loss / config.gradient_accumulation_steps
                loss.backward()

            # Optimizer step
            if (step + 1) % config.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
                for optimizer in optimizers:
                    optimizer.step()
                    optimizer.zero_grad()
                for scheduler in schedulers:
                    scheduler.step()

            # Track current loss as a scalar only every 100 steps to avoid sync bottleneck
            if step % 100 == 0 or step == 0:
                current_loss_val = ce_loss.item()
                
            # Logging
            if step % log_every == 0 or stopped_early:
                with torch.no_grad():
                    # Calculate accuracy using the shifted labels mask
                    predictions = logits.argmax(dim=-1)
                    mask = (shift_labels != -100)
                    accuracy = (predictions[mask] == shift_labels[mask]).float().mean().item()
                    
                    # Use the scalar value we polled above
                    perplexity = math.exp(min(current_loss_val if 'current_loss_val' in locals() else ce_loss.item(), 20))
                    current_lr = schedulers[0].get_last_lr()[0] if schedulers else optimizers[0].param_groups[0]['lr']

                # Update progress bar
                tokens_per_step = config.batch_size * config.max_seq_len * config.gradient_accumulation_steps
                est_total_steps = config.train_tokens // tokens_per_step
                
                pbar.set_postfix({
                    'step': f'{step}/{est_total_steps}',
                    'loss': f'{current_loss_val:.4f}',
                    'acc': f'{accuracy:.3f}',
                    'lr': f'{current_lr:.5f}'
                })
                # Console print for visibility
                if step % (log_every * 10) == 0 or stopped_early:
                    print(f" [Step {step}] Loss: {current_loss_val:.4f} | Acc: {accuracy:.3f} | LR: {current_lr:.6f}")

                # Manifold Tracking
                if track_manifold:
                    with torch.no_grad():
                        # Track all layers for heatmaps
                        num_layers = len(model.transformer_blocks)
                        for i, block in enumerate(model.transformer_blocks):
                            # Access the merged qkvo_proj
                            proj = block.attention.qkvo_proj
                            q_size = block.attention.q_size
                            kv_size = block.attention.kv_size
                            
                            w_q = proj[:q_size]
                            w_k = proj[q_size : q_size + kv_size]
                            w_v = proj[q_size + kv_size : q_size + 2 * kv_size]
                            
                            q_stats = compute_spectral_stats(w_q)
                            k_stats = compute_spectral_stats(w_k)
                            v_stats = compute_spectral_stats(w_v)
                            o_stats = compute_spectral_stats(proj[block.attention.qkv_size:])
                            
                            up_stats = compute_spectral_stats(block.feed_forward.up_proj.weight)
                            down_stats = compute_spectral_stats(block.feed_forward.down_proj.weight)
                            
                            metrics_history['manifold_history'][f'spec_norm_Q_{i}'].append(q_stats['max'])
                            metrics_history['manifold_history'][f'spec_norm_K_{i}'].append(k_stats['max'])
                            metrics_history['manifold_history'][f'spec_norm_V_{i}'].append(v_stats['max'])
                            metrics_history['manifold_history'][f'spec_norm_O_{i}'].append(o_stats['max'])
                            metrics_history['manifold_history'][f'spec_norm_Up_{i}'].append(up_stats['max'])
                            metrics_history['manifold_history'][f'spec_norm_Down_{i}'].append(down_stats['max'])
                            
                            # Log Spectral Entropy (Capacity Allocation) for Q and V
                            metrics_history['manifold_history'][f'entropy_Q_{i}'].append(compute_spectral_entropy(w_q))
                            metrics_history['manifold_history'][f'entropy_V_{i}'].append(compute_spectral_entropy(w_v))
                            
                            # Log Orthogonality Error (Manifold Departure)
                            metrics_history['manifold_history'][f'ortho_err_Q_{i}'].append(compute_orthogonality_error(w_q))
                            metrics_history['manifold_history'][f'ortho_err_V_{i}'].append(compute_orthogonality_error(w_v))

                            # Log Update-Weight Alignment (Geometric Lock-in)
                            muon_opt = optimizers[0]
                            if proj in muon_opt.state and 'last_update' in muon_opt.state[proj]:
                                delta_proj = muon_opt.state[proj]['last_update'].to(proj.device)
                                delta_q = delta_proj[:q_size]
                                delta_v = delta_proj[q_size + kv_size : q_size + 2 * kv_size]
                                
                                metrics_history['manifold_history'][f'alignment_Q_{i}'].append(compute_subspace_alignment(w_q, delta_q, k=5))
                                metrics_history['manifold_history'][f'alignment_V_{i}'].append(compute_subspace_alignment(w_v, delta_v, k=5))
                                
                                # Track Update Rank (The "Needle vs Wave" hypothesis)
                                if proj in muon_opt.state and 'momentum_buffer' in muon_opt.state[proj]:
                                    buf = muon_opt.state[proj]['momentum_buffer']
                                    buf_q = buf[:q_size]
                                    buf_v = buf[q_size + kv_size : q_size + 2 * kv_size]
                                    
                                    from utils.spectral import compute_effective_rank
                                    metrics_history['manifold_history'][f'update_rank_Q_{i}'].append(compute_effective_rank(buf_q))
                                    metrics_history['manifold_history'][f'update_rank_V_{i}'].append(compute_effective_rank(buf_v))
                            
                            # Track detailed stats for first and last layers
                            if i == 0 or i == num_layers - 1:
                                suffix = "0" if i == 0 else "last"
                                metrics_history['manifold_history'][f'spec_gap_Q_{suffix}'].append(q_stats['gap'])
                                metrics_history['manifold_history'][f'spec_vals_Q_{suffix}'].append(compute_singular_values(w_q, n=10))
                        
                        # Add loss and steps to manifold history for synchronization
                        metrics_history['manifold_history']['loss'].append(current_loss_val)
                        metrics_history['manifold_history']['steps'].append(step)
            
            pbar.update(batch_tokens)
            tokens_seen += batch_tokens

            if stopped_early:
                current_loss_val = ce_loss.item()
                break

            # Evaluation
            is_milestone = False
            if config.eval_milestones and step in config.eval_milestones:
                is_milestone = True
            elif config.eval_every is not None and step % config.eval_every == 0 and step > 0:
                is_milestone = True

            if is_milestone:
                eval_metrics = evaluate_model(model, val_loader, config)
                elapsed_time = (time.time() - train_start_time) / 60
                current_lr = schedulers[0].get_last_lr()[0] if schedulers else optimizers[0].param_groups[0]['lr']
                
                # Track metrics
                metrics_history['steps'].append(step)
                metrics_history['val_losses'].append(eval_metrics['val_loss'])
                metrics_history['val_accuracies'].append(eval_metrics['val_accuracy'])
                metrics_history['val_perplexities'].append(eval_metrics['val_perplexity'])
                metrics_history['elapsed_times'].append(elapsed_time)
                metrics_history['learning_rates'].append(current_lr)
                
                print(f"\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, "
                      f"Val Acc: {eval_metrics['val_accuracy']:.4f}, "
                      f"Val PPL: {eval_metrics['val_perplexity']:.2f}, "
                      f"LR: {current_lr:.5f}")
                
                # Early stopping check
                if early_stopper is not None:
                    if early_stopper(eval_metrics['val_loss'], step):
                        current_loss_val = ce_loss.item()
                        stopped_early = True
                        break

            # Save periodic checkpoint
            if checkpoint_dir and step > 0 and step % checkpoint_every == 0:
                ckpt_dir = Path(checkpoint_dir)
                ckpt_dir.mkdir(parents=True, exist_ok=True)
                ckpt_path = ckpt_dir / "latest_checkpoint.pt"
                
                # We save a minimal set needed for resuming
                torch.save({
                    'step': step,
                    'tokens_seen': tokens_seen,
                    'model_state_dict': model.state_dict(),
                    'optimizer_states': [opt.state_dict() for opt in optimizers],
                    'scheduler_states': [sch.state_dict() for sch in schedulers] if schedulers else [],
                    'metrics_history': metrics_history,
                }, ckpt_path)
                # print(f"   üíæ Checkpoint saved at step {step}")

            step += 1
        
        # If we finished the inner loop but didn't stop early, 
        # ensure we have the most recent loss from the very last batch
        if not stopped_early and 'ce_loss' in locals():
            current_loss_val = ce_loss.item()

        if stopped_early:
            break

    pbar.close()

    # Final evaluation (if not stopped early)
    if not stopped_early or tokens_seen >= config.train_tokens:
        final_eval = evaluate_model(model, val_loader, config)
        final_eval['train_loss'] = current_loss_val
        elapsed_time = (time.time() - train_start_time) / 60
        current_lr = schedulers[0].get_last_lr()[0] if schedulers else optimizers[0].param_groups[0]['lr']
        
        metrics_history['steps'].append(step)
        metrics_history['val_losses'].append(final_eval['val_loss'])
        metrics_history['val_accuracies'].append(final_eval['val_accuracy'])
        metrics_history['val_perplexities'].append(final_eval['val_perplexity'])
        metrics_history['elapsed_times'].append(elapsed_time)
        metrics_history['learning_rates'].append(current_lr)
    else:
        # Use best metrics if stopped early
        if metrics_history['val_losses']:
            best_idx = metrics_history['val_losses'].index(min(metrics_history['val_losses']))
            final_eval = {
                'val_loss': metrics_history['val_losses'][best_idx],
                'val_accuracy': metrics_history['val_accuracies'][best_idx],
                'val_perplexity': metrics_history['val_perplexities'][best_idx],
                'train_loss': current_loss_val if 'current_loss_val' in locals() else 0.0,
            }
        else:
            final_eval = {
                'val_loss': current_loss_val if 'current_loss_val' in locals() else 0.0,
                'val_accuracy': accuracy if 'accuracy' in locals() else 0.0,
                'val_perplexity': perplexity if 'perplexity' in locals() else 0.0,
                'train_loss': current_loss_val if 'current_loss_val' in locals() else 0.0,
            }
    
    # Synchronize CUDA to ensure all operations are complete before ending timer
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    total_time_seconds = time.time() - train_start_time
    
    if stopped_early:
        print(f"   ‚ö†Ô∏è  Training stopped early at step {step}")
    
    # Save outputs if directory specified
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save metrics
        metrics_file = output_path / "metrics.json"
        metrics_data = {
            'final_metrics': final_eval,
            'total_time_minutes': total_time_seconds / 60,
            'stopped_early': stopped_early,
            'actual_steps': step,
            'history': metrics_history,
        }
        if extra_config:
            metrics_data['experiment_config'] = extra_config
            
        with open(metrics_file, 'w') as f:
            json.dump(metrics_data, f, indent=2)
        print(f"   üìÅ Metrics saved to {metrics_file}")
        
        
        # Save model checkpoint
        checkpoint_path = output_path / "model.pt"
        resumable_path = Path(checkpoint_dir if checkpoint_dir else output_dir) / "latest_checkpoint.pt"
        
        checkpoint_data = {
            'step': step,
            'tokens_seen': tokens_seen,
            'model_state_dict': model.state_dict(),
            'optimizer_states': [opt.state_dict() for opt in optimizers],
            'scheduler_states': [sch.state_dict() for sch in schedulers] if schedulers else [],
            'metrics_history': metrics_history,
        }
        
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'metrics': final_eval,
            'step': step,
        }, checkpoint_path)
        
        # Also update the latest_checkpoint.pt so we can resume
        torch.save(checkpoint_data, resumable_path)
        
        print(f"   üíæ Final model saved to {checkpoint_path}")
        print(f"   üîÑ Resumable checkpoint saved to {resumable_path}")
    
    return {
        'model': model,
        'final_metrics': final_eval,
        'metrics_history': metrics_history,
        'training_time': total_time_seconds,
        'steps': step,
        'tokens_seen': tokens_seen,
        'train_loss': current_loss_val if 'current_loss_val' in locals() else 0.0,
    }



def warmup_compiled_kernels(
    model: nn.Module,
    config: BlueberryConfig,
    train_loader: DataLoader,
    device: torch.device,
    num_steps: int = 3
) -> None:
    """
    Warm up all compiled kernels (forward, backward, optimizer).
    Caller is responsible for resetting state afterwards.
    """
    print(f"üî• Warming up kernels ({num_steps} steps)...")
    model.train()
    
    # Temporary optimizer to warm up optimizer kernels too
    temp_optimizers = setup_muon_optimizer(model, config)
    
    warmup_iter = iter(train_loader)
    
    for _ in range(num_steps):
        try:
            batch = next(warmup_iter)
        except StopIteration:
            warmup_iter = iter(train_loader)
            batch = next(warmup_iter)
        
        # Parse batch
        if isinstance(batch, dict):
            x, y = batch["input_ids"].to(device), batch["labels"].to(device)
        else:
            x, y = batch[0].to(device), batch[-1].to(device)
        
        # Forward + Backward
        if config.use_amp:
            with autocast('cuda', dtype=torch.bfloat16):
                logits = model(x)
                loss = F.cross_entropy(
                    logits[:, :-1, :].reshape(-1, config.vocab_size),
                    y[:, 1:].reshape(-1)
                )
            loss.backward()
        else:
            logits = model(x)
            loss = F.cross_entropy(
                logits[:, :-1, :].reshape(-1, config.vocab_size),
                y[:, 1:].reshape(-1)
            )
            loss.backward()
        
        # Optimizer step (warms up optimizer kernels)
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
        for opt in temp_optimizers:
            opt.step()
            opt.zero_grad()
    
    torch.cuda.synchronize()
    
    # Cleanup temp optimizers
    del temp_optimizers
    torch.cuda.empty_cache()
    
    print("‚úÖ Kernels compiled and cached")

def train_minimal_llm(
    config: BlueberryConfig,
    train_loader: DataLoader,
    val_loader: DataLoader,
    output_dir: Optional[str] = None,
    load_weights_path: Optional[str] = None,
    compare_baseline: bool = False,
    track_manifold: bool = False,
    resume: bool = False,
    checkpoint_dir: str = "checkpoints",
):
    print(f"\nüöÄ Training dense model")
    setup_start = time.time()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ============================================
    # 1. Initialize model with fixed seed
    # ============================================
    set_seed(42)
    model = MinimalLLM(config)
    model = model.to(device)
    
    # Load pretrained weights if specified
    if load_weights_path:
        print(f"Loading pretrained weights from {load_weights_path}...")
        checkpoint = torch.load(load_weights_path, map_location=device, weights_only=False)
        state_dict = checkpoint.get("model_state_dict", checkpoint)
        model.load_state_dict(state_dict, strict=False)

    # ============================================
    # 2. Save initial state BEFORE any forward pass (cloned to CPU)
    # ============================================
    initial_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  üìä Total parameters: {total_params:,}")

    # ============================================
    # 3. Compile model (if requested)
    # ============================================
    if config.compile_model:
        print("üöÄ Compiling model with torch.compile...")
        # Keep a reference to the original model for state restoration
        orig_model = model
        try:
            model = torch.compile(model)
            print("‚úÖ Model compiled successfully")
            
            # ============================================
            # 4. Warm up kernels (dirties model state)
            # ============================================
            warmup_compiled_kernels(model, config, train_loader, device, num_steps=3)
            
            # ============================================
            # 5. Reset model to initial state
            # ============================================
            # Restore state ensuring we use the original model keys to avoid calling load_state_dict on the wrapper
            orig_model.load_state_dict(initial_model_state)
            print("üîÑ Model weights reset to initial state")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Compilation failed: {e}")
            print("Continuing in eager mode.")
            # Fallback to original model
            model = orig_model
            # Ensure state is clean
            model.load_state_dict(initial_model_state)
    
    # Free the backup
    del initial_model_state
    torch.cuda.empty_cache()

    # ============================================
    # 6. Create FRESH optimizers (no accumulated state)
    # ============================================
    optimizers = setup_muon_optimizer(model, config)

    # ============================================
    # 7. Create FRESH schedulers
    # ============================================
    # Tokens per optimization step
    tokens_per_opt = config.batch_size * config.max_seq_len * config.gradient_accumulation_steps
    total_steps = config.train_tokens // tokens_per_opt
    warmup_steps = max(1, int(total_steps * config.warmup_ratio))
    schedule_type = getattr(config, 'schedule_type', 'cosine')
    
    schedulers = []
    for optimizer in optimizers:
        if schedule_type == 'cosine':
            def lr_lambda(current_step, warmup=warmup_steps, total=total_steps):
                if current_step < warmup:
                    return current_step / warmup
                progress = (current_step - warmup) / max(1, total - warmup)
                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))
        elif schedule_type == 'linear':
            def lr_lambda(current_step, warmup=warmup_steps, total=total_steps):
                if current_step < warmup:
                    return current_step / warmup
                progress = (current_step - warmup) / max(1, total - warmup)
                return max(0.1, 1.0 - progress)
        else:  # constant
            def lr_lambda(current_step, warmup=warmup_steps):
                return current_step / warmup if current_step < warmup else 1.0
        
        schedulers.append(torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda))

    # ============================================
    # 8. Reset RNG for reproducible training
    # ============================================
    set_seed(42)
    
    setup_time = time.time() - setup_start
    print(f"‚öôÔ∏è Setup & Compilation complete in {setup_time:.2f}s")
    print("-" * 70)

    # ============================================
    # 9. Train from scratch (fresh iterator created internally)
    # ============================================
    # Clear GPU cache and synchronize to ensure consistent starting state
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    train_start = time.time()
    
    results = train_model(
        model=model,
        config=config,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizers=optimizers,
        schedulers=schedulers,
        early_stopper=None,
        output_dir=None,
        extra_config=None,
        log_every=getattr(config, 'log_every', 100),
        track_manifold=track_manifold,
        resume_from=os.path.join(checkpoint_dir, "latest_checkpoint.pt") if resume else None,
        checkpoint_dir=checkpoint_dir,
        checkpoint_every=getattr(config, 'save_every', 2000)
    )
    
    total_training_time = results['training_time']
    total_wall_time = setup_time + total_training_time
    final_eval = results['final_metrics']
    metrics_history = results['metrics_history']
    step = results['steps']
    tokens_seen = results['tokens_seen']

    # ============================================
    # 10. Unified Saving & Reporting
    # ============================================
    # Save results to plots/ directory with timestamped names
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    plot_dir = Path("plots")
    plot_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate unique filenames
    base_filename = f"{config.train_tokens}_{timestamp}"
    metrics_file = plot_dir / f"metrics_{base_filename}.json"
    plot_file = plot_dir / f"val_loss_{base_filename}.png"
    
    # Save comprehensive metrics to plots/
    metrics_data = {
        'final_metrics': final_eval,
        'setup_time_seconds': setup_time,
        'active_training_time_seconds': total_training_time,
        'total_wall_time_seconds': total_wall_time,
        'total_time_minutes': total_wall_time / 60,
        'actual_steps': step,
        'tokens_seen': tokens_seen,
        'train_tokens': config.train_tokens,
        'history': metrics_history,
    }
    with open(metrics_file, 'w') as f:
        json.dump(metrics_data, f, indent=2)
    print(f"   üìä Metrics saved to {metrics_file}")
        
    try:
        from utils.plot_loss import plot_loss
        
        baseline_file = None
        if compare_baseline:
            # Determine closest baseline file based on token count
            known_baselines = {
                8_000_000: "plots/8M.json",
                20_000_000: "plots/20M.json",
                100_000_000: "plots/100M.json"
            }
            
            # Find closest baseline
            closest_tokens = min(known_baselines.keys(), key=lambda x: abs(x - config.train_tokens))
            baseline_file = known_baselines[closest_tokens]
            
            # Verify it exists
            if not os.path.exists(baseline_file):
                print(f"      (Baseline file {baseline_file} not found locally)")
                baseline_file = None
            
        plot_loss(
            str(metrics_file), 
            str(plot_file), 
            title=f"Validation Loss - {config.train_tokens:,} Tokens",
            baseline_file=baseline_file
        )
        print(f"   üìà Plot saved to {plot_file}")
        if baseline_file:
            print(f"      (Compared against baseline: {baseline_file})")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Failed to generate plot: {e}")
    
    # Also save to output_dir if specified (for backward compatibility)
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save metrics copy
        checkpoint_metrics = output_path / "metrics.json"
        with open(checkpoint_metrics, 'w') as f:
            json.dump(metrics_data, f, indent=2)
            
        # Save model
        checkpoint_path = output_path / "model.pt"
        torch.save({
            'model_state_dict': results['model'].state_dict(),
            'config': config,
            'metrics': final_eval,
        }, checkpoint_path)
        
    
    # Final Output
    print("\n" + "="*70)
    print(" SPEEDRUN RESULTS")
    print("="*70)
    print(f"Warmup & Setup:                  {format_time(setup_time)}")
    print(f"Training Time (‚è±Ô∏è Speedrun):      {format_time(total_training_time)}")
    print(f"Total Tokens:                    {tokens_seen:,}")
    print("-" * 70)
    print(f"Final Train Loss:                {final_eval.get('train_loss', 0.0):.4f}")
    print(f"Final Val Loss:                  {final_eval['val_loss']:.4f}")
    print(f"Final Val Accuracy:              {final_eval['val_accuracy']:.4f}")
    print("="*70 + "\n")

    return {
        'model': results['model'],
        'metrics': final_eval,
        'history': metrics_history,
        'setup_time': setup_time,
        'training_time': total_training_time,
        'steps': step,
        'tokens_seen': tokens_seen
    }